{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sknetwork.ranking import PageRank\n",
    "import pickle\n",
    "from causallearn.search.FCMBased import lingam\n",
    "from causallearn.utils.cit import chisq, fisherz, gsq, kci, mv_fisherz\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import time\n",
    "import networkx as nx\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T18:40:08.781284Z",
     "start_time": "2024-12-13T18:40:08.777671Z"
    }
   },
   "id": "f036c4af484f25cf",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DiWalk:\n",
    "    def __init__(self, graph, start_node, rho=0.5):\n",
    "        \"\"\"\n",
    "        Initialize AutoMAP with graph GA, starting node, and rho parameter.\n",
    "        \n",
    "        :param graph: networkx.DiGraph representing the service-calling topology\n",
    "        :param start_node: Node from which the random walk starts\n",
    "        :param rho: Parameter controlling backward transition strength (0 <= rho < 1)\n",
    "        \"\"\"\n",
    "        self.GA = graph\n",
    "        if start_node not in self.GA:\n",
    "            raise ValueError(f\"Start node '{start_node}' is not in the graph.\")\n",
    "        self.start_node = start_node\n",
    "        if not (0 <= rho < 1):\n",
    "            raise ValueError(\"Parameter rho must be in the range [0, 1).\")\n",
    "        self.rho = rho\n",
    "        self.visit_counts = defaultdict(int)\n",
    "    \n",
    "    def random_walk(self, steps=100, seed=None):\n",
    "        \"\"\"\n",
    "        Perform random walk for a given number of steps.\n",
    "        \n",
    "        :param steps: Number of steps to simulate\n",
    "        :param seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        \n",
    "        current_node = self.start_node\n",
    "        self.visit_counts[current_node] += 1\n",
    "        \n",
    "        for step in range(steps):\n",
    "            Oi = list(self.GA.successors(current_node))\n",
    "            Ii = list(self.GA.predecessors(current_node))\n",
    "            \n",
    "            # Assign weights\n",
    "            forward_weight = len(Oi)  # Each forward transition has weight=1\n",
    "            backward_weight = len(Ii) * self.rho  # Each backward transition has weight=rho\n",
    "            self_weight = 1  # Self-transition weight=1\n",
    "            \n",
    "            total_weight = forward_weight + backward_weight + self_weight\n",
    "            if total_weight == 0:\n",
    "                # If no transitions are possible, stay on the current node\n",
    "                self.visit_counts[current_node] += 1\n",
    "                continue\n",
    "            \n",
    "            probabilities = []\n",
    "            nodes = []\n",
    "            \n",
    "            # Forward transitions\n",
    "            for neighbor in Oi:\n",
    "                probabilities.append(1 / total_weight)\n",
    "                nodes.append(neighbor)\n",
    "            \n",
    "            # Backward transitions\n",
    "            for neighbor in Ii:\n",
    "                probabilities.append(self.rho / total_weight)\n",
    "                nodes.append(neighbor)\n",
    "            \n",
    "            # Self-transition\n",
    "            probabilities.append(self_weight / total_weight)\n",
    "            nodes.append(current_node)\n",
    "            \n",
    "            # Choose next node based on probabilities\n",
    "            next_node = random.choices(nodes, weights=probabilities, k=1)[0]\n",
    "            self.visit_counts[next_node] += 1\n",
    "            current_node = next_node\n",
    "    \n",
    "    def get_sorted_visit_counts(self):\n",
    "        \"\"\"\n",
    "        Get the sorted list of nodes based on visit counts.\n",
    "        \n",
    "        :return: List of tuples sorted by visit counts in descending order\n",
    "        \"\"\"\n",
    "        sorted_counts = sorted(self.visit_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_counts\n",
    "\n",
    "    def reset_counts(self):\n",
    "        \"\"\"\n",
    "        Reset the visit counts.\n",
    "        \"\"\"\n",
    "        self.visit_counts = defaultdict(int)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T18:40:14.301327Z",
     "start_time": "2024-12-13T18:40:14.295535Z"
    }
   },
   "id": "initial_id",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hts_res_dict = {}\n",
    "for experiment in range(0,4):\n",
    "    with open(f\"../data/fault_data/hts_fault_{experiment}.pkl\", \"rb\") as f:\n",
    "        s_list, X = pickle.load(f)\n",
    "    ind = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[ind] = x\n",
    "        ind+=1\n",
    "    # Start CausalRCA\n",
    "    X = np.diff(X,axis=0)\n",
    "    X = normalize(X,axis=0)\n",
    "    start_time = time.time()\n",
    "    model = lingam.DirectLiNGAM()\n",
    "    model.fit(X)\n",
    "    adj = model.adjacency_matrix_.T \n",
    "    G = nx.from_numpy_array(adj, create_using=nx.DiGraph)\n",
    "    G = G.reverse(copy=True)\n",
    "    total_visit_counts = defaultdict(int)\n",
    "    steps_per_walk = 10\n",
    "    for i in range(46):\n",
    "        for rep in range(0,100):\n",
    "            walker = DiWalk(graph=G, start_node=i, rho=0)\n",
    "            walker.random_walk(steps=steps_per_walk)\n",
    "            \n",
    "            # Accumulate the visit counts from this walk\n",
    "            for node, count in walker.visit_counts.items():\n",
    "                total_visit_counts[node] += count\n",
    "    \n",
    "    total_f = total_visit_counts\n",
    "    combined_frequencies = defaultdict(float)\n",
    "    \n",
    "    i = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[i] = x\n",
    "        i+=1\n",
    "        \n",
    "    for service_id, frequency in total_f.items():\n",
    "        service_type = mapping.get(service_id, 'unknown')\n",
    "        combined_frequencies[service_type] += frequency\n",
    "    \n",
    "    # Convert the combined frequencies dictionary to a pandas dataframe\n",
    "    combined_frequencies_df = pd.DataFrame(\n",
    "        list(combined_frequencies.items()), columns=[\"Service Type\", \"Combined Frequency\"]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    # ####\n",
    "    # \n",
    "    t_elapsed = end_time - start_time\n",
    "    sorted_pr = combined_frequencies_df.sort_values(by=\"Combined Frequency\", ascending=False)\n",
    "    # top_5 = list(sorted_pr.head(5)[\"Service Type\"])\n",
    "    hts_res_dict[experiment] = (sorted_pr, t_elapsed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T18:41:29.940924Z",
     "start_time": "2024-12-13T18:40:30.372141Z"
    }
   },
   "id": "be734e93fe973aac",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "adff6e88824f0095"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cp_res_dict = {}\n",
    "for experiment in range(0,4):\n",
    "    with open(f\"../data/fault_data/cp_fault_{experiment}.pkl\", \"rb\") as f:\n",
    "        s_list, X = pickle.load(f)\n",
    "    ind = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[ind] = x\n",
    "        ind+=1\n",
    "    # Start CausalRCA\n",
    "    X = np.diff(X,axis=0)\n",
    "    X = normalize(X,axis=0)\n",
    "    start_time = time.time()\n",
    "    model = lingam.DirectLiNGAM()\n",
    "    model.fit(X)\n",
    "    adj = model.adjacency_matrix_.T \n",
    "    G = nx.from_numpy_array(adj, create_using=nx.DiGraph)\n",
    "    G = G.reverse(copy=True)\n",
    "    total_visit_counts = defaultdict(int)\n",
    "    steps_per_walk = 10\n",
    "    for i in range(46):\n",
    "        for rep in range(0,100):\n",
    "            walker = DiWalk(graph=G, start_node=i, rho=0.1)\n",
    "            walker.random_walk(steps=steps_per_walk)\n",
    "            \n",
    "            # Accumulate the visit counts from this walk\n",
    "            for node, count in walker.visit_counts.items():\n",
    "                total_visit_counts[node] += count\n",
    "    \n",
    "    total_f = total_visit_counts\n",
    "    combined_frequencies = defaultdict(float)\n",
    "    \n",
    "    i = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[i] = x\n",
    "        i+=1\n",
    "        \n",
    "    for service_id, frequency in total_f.items():\n",
    "        service_type = mapping.get(service_id, 'unknown')\n",
    "        combined_frequencies[service_type] += frequency\n",
    "    \n",
    "    # Convert the combined frequencies dictionary to a pandas dataframe\n",
    "    combined_frequencies_df = pd.DataFrame(\n",
    "        list(combined_frequencies.items()), columns=[\"Service Type\", \"Combined Frequency\"]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    # ####\n",
    "    # \n",
    "    t_elapsed = end_time - start_time\n",
    "    sorted_pr = combined_frequencies_df.sort_values(by=\"Combined Frequency\", ascending=False)\n",
    "    # top_5 = list(sorted_pr.head(5)[\"Service Type\"])\n",
    "    cp_res_dict[experiment] = (sorted_pr, t_elapsed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T18:54:17.252806Z",
     "start_time": "2024-12-13T18:53:17.519099Z"
    }
   },
   "id": "9a994dd855e7de0b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c2e10553ade3d8da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
