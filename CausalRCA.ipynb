{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Giles\\AppData\\Local\\Temp\\ipykernel_35816\\1951157130.py:27: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  os.chdir(\"D:\\Documents\\Code\\dsb-investigate\\models\\CausalRCA_code\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "import os\n",
    "\n",
    "os.chdir(\"CausalRCA_code\")\n",
    "from utils import *\n",
    "from modules import *\n",
    "#from paras import *\n",
    "from config import CONFIG\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "from sknetwork.ranking import PageRank\n",
    "from sklearn.preprocessing import normalize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T13:20:56.021194Z",
     "start_time": "2024-12-18T13:20:51.026894Z"
    }
   },
   "id": "558c1ba3325dfd80",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "CONFIG.cuda = torch.cuda.is_available()\n",
    "CONFIG.factor = not CONFIG.no_factor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T13:20:56.025620Z",
     "start_time": "2024-12-18T13:20:56.022217Z"
    }
   },
   "id": "cdde527d1972db7c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def _h_A(A, m):\n",
    "    expm_A = matrix_poly(A*A, m)\n",
    "    h_A = torch.trace(expm_A) - m\n",
    "    return h_A\n",
    "\n",
    "def stau(w, tau):\n",
    "    w1 = prox_plus(torch.abs(w)-tau)\n",
    "    return torch.sign(w)*w1\n",
    "\n",
    "def update_optimizer(optimizer, original_lr, c_A):\n",
    "    '''related LR to c_A, whenever c_A gets big, reduce LR proportionally'''\n",
    "    MAX_LR = 1e-2\n",
    "    MIN_LR = 1e-4\n",
    "\n",
    "    estimated_lr = original_lr / (math.log10(c_A) + 1e-10)\n",
    "    if estimated_lr > MAX_LR:\n",
    "        lr = MAX_LR\n",
    "    elif estimated_lr < MIN_LR:\n",
    "        lr = MIN_LR\n",
    "    else:\n",
    "        lr = estimated_lr\n",
    "\n",
    "    # set LR\n",
    "    for parame_group in optimizer.param_groups:\n",
    "        parame_group['lr'] = lr\n",
    "\n",
    "    return optimizer, lr\n",
    "\n",
    "def train(epoch, best_val_loss, lambda_A, c_A, optimizer):\n",
    "    t = time.time()\n",
    "    nll_train = []\n",
    "    kl_train = []\n",
    "    mse_train = []\n",
    "    shd_trian = []\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    scheduler.step()\n",
    "\n",
    "    # update optimizer\n",
    "    optimizer, lr = update_optimizer(optimizer, CONFIG.lr, c_A)\n",
    "\n",
    "    for i in range(1):\n",
    "        data = train_data[i*data_sample_size:(i+1)*data_sample_size]\n",
    "        data = torch.tensor(data.to_numpy().reshape(data_sample_size,data_variable_size,1))\n",
    "        if CONFIG.cuda:\n",
    "            data = data.cuda()\n",
    "        data = Variable(data).double()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        enc_x, logits, origin_A, adj_A_tilt_encoder, z_gap, z_positive, myA, Wa = encoder(data)  # logits is of size: [num_sims, z_dims]\n",
    "        edges = logits\n",
    "        #print(origin_A)\n",
    "        dec_x, output, adj_A_tilt_decoder = decoder(data, edges, data_variable_size * CONFIG.x_dims, origin_A, adj_A_tilt_encoder, Wa)\n",
    "\n",
    "        if torch.sum(output != output):\n",
    "            print('nan error\\n')\n",
    "\n",
    "        target = data\n",
    "        preds = output\n",
    "        variance = 0.\n",
    "\n",
    "        # reconstruction accuracy loss\n",
    "        loss_nll = nll_gaussian(preds, target, variance)\n",
    "\n",
    "        # KL loss\n",
    "        loss_kl = kl_gaussian_sem(logits)\n",
    "\n",
    "        # ELBO loss:\n",
    "        loss = loss_kl + loss_nll\n",
    "        # add A loss\n",
    "        one_adj_A = origin_A # torch.mean(adj_A_tilt_decoder, dim =0)\n",
    "        sparse_loss = CONFIG.tau_A * torch.sum(torch.abs(one_adj_A))\n",
    "\n",
    "        # other loss term\n",
    "        if CONFIG.use_A_connect_loss:\n",
    "            connect_gap = A_connect_loss(one_adj_A, CONFIG.graph_threshold, z_gap)\n",
    "            loss += lambda_A * connect_gap + 0.5 * c_A * connect_gap * connect_gap\n",
    "\n",
    "        if CONFIG.use_A_positiver_loss:\n",
    "            positive_gap = A_positive_loss(one_adj_A, z_positive)\n",
    "            loss += .1 * (lambda_A * positive_gap + 0.5 * c_A * positive_gap * positive_gap)\n",
    "\n",
    "        # compute h(A)\n",
    "        h_A = _h_A(origin_A, data_variable_size)\n",
    "        loss += lambda_A * h_A + 0.5 * c_A * h_A * h_A + 100. * torch.trace(origin_A*origin_A) + sparse_loss #+  0.01 * torch.sum(variance * variance)\n",
    "        \n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        loss = optimizer.step()\n",
    "\n",
    "        myA.data = stau(myA.data, CONFIG.tau_A*lr)\n",
    "\n",
    "        if torch.sum(origin_A != origin_A):\n",
    "            print('nan error\\n')\n",
    "\n",
    "        # compute metrics\n",
    "        graph = origin_A.data.clone().cpu().numpy()\n",
    "        graph[np.abs(graph) < CONFIG.graph_threshold] = 0\n",
    "\n",
    "        mse_train.append(F.mse_loss(preds, target).item())\n",
    "        nll_train.append(loss_nll.item())\n",
    "        kl_train.append(loss_kl.item())\n",
    "\n",
    "    return np.mean(np.mean(kl_train)  + np.mean(nll_train)), np.mean(nll_train), np.mean(mse_train), graph, origin_A"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-18T13:20:56.035597Z",
     "start_time": "2024-12-18T13:20:56.026626Z"
    }
   },
   "id": "initial_id",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "996988aab360444c859e152ed8780ff6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Code\\dsb-investigate\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "054922c1dcd64eb28d4dd498874f2aad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Code\\dsb-investigate\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45de4ffca10f4756b97180f70f3e9fa1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Code\\dsb-investigate\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9c366f61eb9449fb7d868521f411657"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Code\\dsb-investigate\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "hts_res_dict = {}\n",
    "for experiment in range(0,4):\n",
    "    with open(f\"D:/Documents/Code/dsb-investigate/data/fault_data/hts_fault_{experiment}.pkl\", \"rb\") as f:\n",
    "        s_list, X = pickle.load(f)\n",
    "    ind = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[ind] = x\n",
    "        ind+=1\n",
    "    \n",
    "    start_time = time.time()\n",
    "    X = np.diff(X,axis=0)\n",
    "    X = normalize(X,axis=0)\n",
    "    data = pd.DataFrame(X, columns=s_list)\n",
    "    data_sample_size = data.shape[0]\n",
    "    data_variable_size = data.shape[1]\n",
    "    train_data = data\n",
    "    off_diag = np.ones([data_variable_size, data_variable_size]) - np.eye(data_variable_size)\n",
    "\n",
    "    # add adjacency matrix A\n",
    "    num_nodes = data_variable_size\n",
    "    adj_A = np.zeros((num_nodes, num_nodes))\n",
    "    encoder = MLPEncoder(data_variable_size * CONFIG.x_dims, CONFIG.x_dims, CONFIG.encoder_hidden,\n",
    "                     int(CONFIG.z_dims), adj_A,\n",
    "                     batch_size = CONFIG.batch_size,\n",
    "                     do_prob = CONFIG.encoder_dropout, factor = CONFIG.factor).double()\n",
    "    \n",
    "    decoder = MLPDecoder(data_variable_size * CONFIG.x_dims,\n",
    "                         CONFIG.z_dims, CONFIG.x_dims, encoder,\n",
    "                         data_variable_size = data_variable_size,\n",
    "                         batch_size = CONFIG.batch_size,\n",
    "                         n_hid=CONFIG.decoder_hidden,\n",
    "                         do_prob=CONFIG.decoder_dropout).double()\n",
    "    \n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=CONFIG.lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=CONFIG.lr_decay,\n",
    "                                gamma=CONFIG.gamma)\n",
    "    # Linear indices of an upper triangular mx, used for acc calculation\n",
    "    triu_indices = get_triu_offdiag_indices(data_variable_size)\n",
    "    tril_indices = get_tril_offdiag_indices(data_variable_size)\n",
    "    \n",
    "    if CONFIG.cuda:\n",
    "        print(\"Using CUDA\")\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()\n",
    "        triu_indices = triu_indices.cuda()\n",
    "        tril_indices = tril_indices.cuda()\n",
    "    \n",
    "    prox_plus = torch.nn.Threshold(0.,0.)\n",
    "    \n",
    "    gamma = 0.25\n",
    "    eta = 10\n",
    "    k_max_iter = 100\n",
    "    best_ELBO_loss = np.inf\n",
    "    best_NLL_loss = np.inf\n",
    "    best_MSE_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    best_ELBO_graph = []\n",
    "    best_NLL_graph = []\n",
    "    best_MSE_graph = []\n",
    "    \n",
    "    c_A = CONFIG.c_A\n",
    "    lambda_A = CONFIG.lambda_A\n",
    "    h_A_new = torch.tensor(1.)\n",
    "    h_tol = CONFIG.h_tol\n",
    "    h_A_old = np.inf\n",
    "    \n",
    "    E_loss = []\n",
    "    N_loss = []\n",
    "    M_loss = []\n",
    "    # start_time = time.time()\n",
    "    try:\n",
    "        for step_k in trange(k_max_iter):\n",
    "            #print(step_k)\n",
    "            while c_A < 1e+20:\n",
    "                for epoch in range(CONFIG.epochs):\n",
    "                    #print(epoch)\n",
    "                    ELBO_loss, NLL_loss, MSE_loss, graph, origin_A = train(epoch, best_ELBO_loss, lambda_A, c_A, optimizer)\n",
    "                    E_loss.append(ELBO_loss)\n",
    "                    N_loss.append(NLL_loss)\n",
    "                    M_loss.append(MSE_loss)\n",
    "                    if ELBO_loss < best_ELBO_loss:\n",
    "                        best_ELBO_loss = ELBO_loss\n",
    "                        best_epoch = epoch\n",
    "                        best_ELBO_graph = graph\n",
    "    \n",
    "                    if NLL_loss < best_NLL_loss:\n",
    "                        best_NLL_loss = NLL_loss\n",
    "                        best_epoch = epoch\n",
    "                        best_NLL_graph = graph\n",
    "    \n",
    "                    if MSE_loss < best_MSE_loss:\n",
    "                        best_MSE_loss = MSE_loss\n",
    "                        best_epoch = epoch\n",
    "                        best_MSE_graph = graph\n",
    "    \n",
    "                #print(\"Optimization Finished!\")\n",
    "                #print(\"Best Epoch: {:04d}\".format(best_epoch))\n",
    "                if ELBO_loss > 2 * best_ELBO_loss:\n",
    "                    break\n",
    "    \n",
    "                # update parameters\n",
    "                A_new = origin_A.data.clone()\n",
    "                h_A_new = _h_A(A_new, data_variable_size)\n",
    "                if h_A_new.item() > gamma * h_A_old:\n",
    "                    c_A*=eta\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "            # update parameters\n",
    "            # h_A, adj_A are computed in loss anyway, so no need to store\n",
    "            h_A_old = h_A_new.item()\n",
    "            lambda_A += c_A * h_A_new.item()\n",
    "    \n",
    "            if h_A_new.item() <= h_tol:\n",
    "                break\n",
    "            \n",
    "        #print(\"Steps: {:04d}\".format(step_k))\n",
    "        #print(\"Best Epoch: {:04d}\".format(best_epoch))\n",
    "    \n",
    "        # test()\n",
    "        #print (best_ELBO_graph)\n",
    "        #print(best_NLL_graph)\n",
    "        #print (best_MSE_graph)\n",
    "    \n",
    "        graph = origin_A.data.clone().cpu().numpy()\n",
    "        graph[np.abs(graph) < 0.01] = 0\n",
    "        # graph[np.abs(graph) < 0.1] = 0\n",
    "        # graph[np.abs(graph) < 0.2] = 0\n",
    "        # graph[np.abs(graph) < 0.3] = 0\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Done!')\n",
    "    adj = graph\n",
    "    # PageRank\n",
    "    pagerank = PageRank()\n",
    "    scores = pagerank.fit_predict(np.abs(adj.T))\n",
    "    score_dict = {}\n",
    "    for i,s in enumerate(scores):\n",
    "        score_dict[i] = s\n",
    "        combined_pagerank = defaultdict(float)\n",
    "    \n",
    "    for service_id in range(len(scores)):\n",
    "        combined_pagerank[mapping[service_id]] += scores[service_id]\n",
    "    \n",
    "    combined_pagerank_df = pd.DataFrame(\n",
    "                list(combined_pagerank.items()), columns=[\"Service Type\", \"Combined Frequency\"]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    t_elapsed = end_time - start_time\n",
    "    sorted_pr = combined_pagerank_df.sort_values(by=\"Combined Frequency\", ascending=False)\n",
    "    top_5 = list(sorted_pr.head(5)[\"Service Type\"])\n",
    "    hts_res_dict[experiment] = (sorted_pr, t_elapsed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T13:45:05.542084Z",
     "start_time": "2024-12-18T13:40:07.255791Z"
    }
   },
   "id": "8ea790900d6768a5",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "adff6e88824f0095"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bf98a39cc754682934ecba58d6f5f5e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Code\\dsb-investigate\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38fb0a69f2c8469698fa1492e60866fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Code\\dsb-investigate\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f63a3bddbb54603a5aa2b849f140b51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Code\\dsb-investigate\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "725e3bfa5ec449b1a1c6a8da0cc867a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Code\\dsb-investigate\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "cp_res_dict = {}\n",
    "for experiment in range(0,4):\n",
    "    with open(f\"D:/Documents/Code/dsb-investigate/data/fault_data/cp_fault_{experiment}.pkl\", \"rb\") as f:\n",
    "        s_list, X = pickle.load(f)\n",
    "    ind = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[ind] = x\n",
    "        ind+=1\n",
    "    \n",
    "    start_time = time.time()\n",
    "    X = np.diff(X,axis=0)\n",
    "    X = normalize(X,axis=0)\n",
    "    data = pd.DataFrame(X, columns=s_list)\n",
    "    data_sample_size = data.shape[0]\n",
    "    data_variable_size = data.shape[1]\n",
    "    train_data = data\n",
    "    off_diag = np.ones([data_variable_size, data_variable_size]) - np.eye(data_variable_size)\n",
    "\n",
    "    # add adjacency matrix A\n",
    "    num_nodes = data_variable_size\n",
    "    adj_A = np.zeros((num_nodes, num_nodes))\n",
    "    encoder = MLPEncoder(data_variable_size * CONFIG.x_dims, CONFIG.x_dims, CONFIG.encoder_hidden,\n",
    "                     int(CONFIG.z_dims), adj_A,\n",
    "                     batch_size = CONFIG.batch_size,\n",
    "                     do_prob = CONFIG.encoder_dropout, factor = CONFIG.factor).double()\n",
    "    \n",
    "    decoder = MLPDecoder(data_variable_size * CONFIG.x_dims,\n",
    "                         CONFIG.z_dims, CONFIG.x_dims, encoder,\n",
    "                         data_variable_size = data_variable_size,\n",
    "                         batch_size = CONFIG.batch_size,\n",
    "                         n_hid=CONFIG.decoder_hidden,\n",
    "                         do_prob=CONFIG.decoder_dropout).double()\n",
    "    \n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=CONFIG.lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=CONFIG.lr_decay,\n",
    "                                gamma=CONFIG.gamma)\n",
    "    # Linear indices of an upper triangular mx, used for acc calculation\n",
    "    triu_indices = get_triu_offdiag_indices(data_variable_size)\n",
    "    tril_indices = get_tril_offdiag_indices(data_variable_size)\n",
    "    \n",
    "    if CONFIG.cuda:\n",
    "        print(\"Using CUDA\")\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()\n",
    "        triu_indices = triu_indices.cuda()\n",
    "        tril_indices = tril_indices.cuda()\n",
    "    \n",
    "    prox_plus = torch.nn.Threshold(0.,0.)\n",
    "    \n",
    "    gamma = 0.25\n",
    "    eta = 10\n",
    "    \n",
    "    best_ELBO_loss = np.inf\n",
    "    best_NLL_loss = np.inf\n",
    "    best_MSE_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    best_ELBO_graph = []\n",
    "    best_NLL_graph = []\n",
    "    best_MSE_graph = []\n",
    "    \n",
    "    c_A = CONFIG.c_A\n",
    "    lambda_A = CONFIG.lambda_A\n",
    "    h_A_new = torch.tensor(1.)\n",
    "    h_tol = CONFIG.h_tol\n",
    "    k_max_iter = int(CONFIG.k_max_iter)\n",
    "    h_A_old = np.inf\n",
    "    \n",
    "    E_loss = []\n",
    "    N_loss = []\n",
    "    M_loss = []\n",
    "    # start_time = time.time()\n",
    "    try:\n",
    "        for step_k in trange(k_max_iter):\n",
    "            #print(step_k)\n",
    "            while c_A < 1e+20:\n",
    "                for epoch in range(CONFIG.epochs):\n",
    "                    #print(epoch)\n",
    "                    ELBO_loss, NLL_loss, MSE_loss, graph, origin_A = train(epoch, best_ELBO_loss, lambda_A, c_A, optimizer)\n",
    "                    E_loss.append(ELBO_loss)\n",
    "                    N_loss.append(NLL_loss)\n",
    "                    M_loss.append(MSE_loss)\n",
    "                    if ELBO_loss < best_ELBO_loss:\n",
    "                        best_ELBO_loss = ELBO_loss\n",
    "                        best_epoch = epoch\n",
    "                        best_ELBO_graph = graph\n",
    "    \n",
    "                    if NLL_loss < best_NLL_loss:\n",
    "                        best_NLL_loss = NLL_loss\n",
    "                        best_epoch = epoch\n",
    "                        best_NLL_graph = graph\n",
    "    \n",
    "                    if MSE_loss < best_MSE_loss:\n",
    "                        best_MSE_loss = MSE_loss\n",
    "                        best_epoch = epoch\n",
    "                        best_MSE_graph = graph\n",
    "    \n",
    "                #print(\"Optimization Finished!\")\n",
    "                #print(\"Best Epoch: {:04d}\".format(best_epoch))\n",
    "                if ELBO_loss > 2 * best_ELBO_loss:\n",
    "                    break\n",
    "    \n",
    "                # update parameters\n",
    "                A_new = origin_A.data.clone()\n",
    "                h_A_new = _h_A(A_new, data_variable_size)\n",
    "                if h_A_new.item() > gamma * h_A_old:\n",
    "                    c_A*=eta\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "            # update parameters\n",
    "            # h_A, adj_A are computed in loss anyway, so no need to store\n",
    "            h_A_old = h_A_new.item()\n",
    "            lambda_A += c_A * h_A_new.item()\n",
    "    \n",
    "            if h_A_new.item() <= h_tol:\n",
    "                break\n",
    "            \n",
    "        #print(\"Steps: {:04d}\".format(step_k))\n",
    "        #print(\"Best Epoch: {:04d}\".format(best_epoch))\n",
    "    \n",
    "        # test()\n",
    "        #print (best_ELBO_graph)\n",
    "        #print(best_NLL_graph)\n",
    "        #print (best_MSE_graph)\n",
    "    \n",
    "        graph = origin_A.data.clone().cpu().numpy()\n",
    "        graph[np.abs(graph) < 0.01] = 0\n",
    "        # graph[np.abs(graph) < 0.1] = 0\n",
    "        # graph[np.abs(graph) < 0.2] = 0\n",
    "        # graph[np.abs(graph) < 0.3] = 0\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Done!')\n",
    "    adj = graph\n",
    "    # PageRank\n",
    "    pagerank = PageRank()\n",
    "    scores = pagerank.fit_predict(np.abs(adj.T))\n",
    "    score_dict = {}\n",
    "    for i,s in enumerate(scores):\n",
    "        score_dict[i] = s\n",
    "        combined_pagerank = defaultdict(float)\n",
    "    \n",
    "    for service_id in range(len(scores)):\n",
    "        combined_pagerank[mapping[service_id]] += scores[service_id]\n",
    "    \n",
    "    combined_pagerank_df = pd.DataFrame(\n",
    "                list(combined_pagerank.items()), columns=[\"Service Type\", \"Combined Frequency\"]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    t_elapsed = end_time - start_time\n",
    "    sorted_pr = combined_pagerank_df.sort_values(by=\"Combined Frequency\", ascending=False)\n",
    "    top_5 = list(sorted_pr.head(5)[\"Service Type\"])\n",
    "    cp_res_dict[experiment] = (sorted_pr, t_elapsed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T14:00:16.409750Z",
     "start_time": "2024-12-18T13:51:42.342099Z"
    }
   },
   "id": "9a994dd855e7de0b",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c2e10553ade3d8da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
