{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import dsbcorr\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import normalize\n",
    "from topcorr import tmfg"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T08:10:26.198243Z",
     "start_time": "2024-12-14T08:10:19.569168Z"
    }
   },
   "id": "593d4deab911f35f",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_service_type(node_label):\n",
    "    return '-'.join(node_label.split('-')[:2])\n",
    "\n",
    "def weighted_random_walk(graph, start_node, walk_length, weight='weight'):\n",
    "    \"\"\"\n",
    "    Perform a single weighted random walk starting from a given node.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: NetworkX graph\n",
    "    - start_node: Node to start the random walk from\n",
    "    - walk_length: Length of the random walk\n",
    "    - weight: Edge attribute to use as weight (default is 'weight')\n",
    "    \"\"\"\n",
    "    path = [start_node]\n",
    "    for _ in range(walk_length - 1):\n",
    "        current_node = path[-1]\n",
    "        neighbors = list(graph.neighbors(current_node))\n",
    "        if not neighbors:  # If no neighbors, end the walk\n",
    "            break\n",
    "        \n",
    "        # Get weights of edges to neighbors\n",
    "        weights = np.array([graph[current_node][neighbor].get(weight, 1) for neighbor in neighbors])\n",
    "        probabilities = weights / weights.sum()  # Normalize to get probabilities\n",
    "        \n",
    "        # Choose the next node based on the probabilities\n",
    "        next_node = random.choices(neighbors, weights=probabilities, k=1)[0]\n",
    "        path.append(next_node)\n",
    "    \n",
    "    return path\n",
    "\n",
    "def characterize_nodes_weighted(graph, walk_length=10, num_walks=100, weight='weight'):\n",
    "    \"\"\"\n",
    "    Characterize all nodes in the graph using weighted random walks.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: NetworkX graph\n",
    "    - walk_length: Length of each random walk\n",
    "    - num_walks: Number of random walks per node\n",
    "    - weight: Edge attribute to use as weight\n",
    "    \n",
    "    Returns:\n",
    "    - node_characteristics: Dictionary of visit frequencies for each node\n",
    "    \"\"\"\n",
    "    node_characteristics = {}\n",
    "    for node in graph.nodes:\n",
    "        # Collect weighted random walk paths starting from this node\n",
    "        all_walks = [weighted_random_walk(graph, node, walk_length, weight) for _ in range(num_walks)]\n",
    "        \n",
    "        # Compute visit frequencies for each node\n",
    "        visit_counts = {}\n",
    "        for walk in all_walks:\n",
    "            for visited_node in walk:\n",
    "                visit_counts[visited_node] = visit_counts.get(visited_node, 0) + 1\n",
    "        \n",
    "        # Normalize frequencies\n",
    "        total_visits = sum(visit_counts.values())\n",
    "        visit_frequencies = {n: count / total_visits for n, count in visit_counts.items()}\n",
    "        \n",
    "        # Store the characteristics\n",
    "        node_characteristics[node] = visit_frequencies\n",
    "    return node_characteristics\n",
    "\n",
    "def get_normal_label(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    max_count_index = np.argmax(counts)\n",
    "    return unique[max_count_index]\n",
    "\n",
    "def get_normal_mat(labels, r_dict, normal_label, fault_label):\n",
    "    first_index = np.where(labels == fault_label)[0][0]\n",
    "    normal_indexes = np.where((labels == normal_label) & (np.arange(len(labels)) < first_index))[0]\n",
    "    avg_network = np.zeros((np.shape(r_dict[0])))\n",
    "    for idx in normal_indexes:\n",
    "        avg_network = avg_network + r_dict[idx]\n",
    "    return avg_network/len(normal_indexes)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T08:10:26.205799Z",
     "start_time": "2024-12-14T08:10:26.199247Z"
    }
   },
   "id": "initial_id",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:54<00:00, 13.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "hts_res_dict = {}\n",
    "for experiment in trange(0,4):\n",
    "    with open(f\"../data/fault_data/hts_fault_{experiment}.pkl\", \"rb\") as f:\n",
    "        s_list, X = pickle.load(f)\n",
    "    ind = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[ind] = x\n",
    "        ind+=1\n",
    "        \n",
    "    # Start FC-FDL\n",
    "    win = 360\n",
    "    step = 1\n",
    "    Xc = deepcopy(X)\n",
    "    Xdiff = np.diff(Xc[:1080,:],axis=0)\n",
    "    Xdiff = normalize(Xdiff,axis=0)\n",
    "    r_dict = dsbcorr.rolling_window(Xdiff[:,:], win, \"tapered\", step, 0.3, \"tmfg\", \"pearsons\")\n",
    "    \n",
    "    r_pos = {}\n",
    "    for corr in r_dict:\n",
    "        r_pos[corr]=deepcopy(r_dict[corr])\n",
    "        r_pos[corr] += 1\n",
    "    \n",
    "    spectra = dsbcorr.all_spectra(r_pos, True)\n",
    "    all_es = spectra.T\n",
    "    all_es = all_es[:,:-1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    landmarks = random.sample(range(0, all_es.shape[0], 1), int(len(all_es) / 4))\n",
    "    dist = distance.cdist(all_es[landmarks, :], all_es, 'euclidean')\n",
    "    coords = dsbcorr.LMDS(dist, landmarks, 2)\n",
    "    \n",
    "    n_clusters = 2\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\").fit(coords)\n",
    "    labels=clustering.labels_\n",
    "    \n",
    "    # Find the order of first instance cluster appearance\n",
    "    order_of_clusters = [key for key, _ in groupby(labels)]\n",
    "    \n",
    "    # Initialize an empty list to store unique clusters in the order of first appearance\n",
    "    unique_clusters = []\n",
    "    \n",
    "    # Set to keep track of seen clusters\n",
    "    seen = set()\n",
    "    \n",
    "    # Iterate through the labels\n",
    "    for label in labels:\n",
    "        if label not in seen:\n",
    "            unique_clusters.append(label)\n",
    "            seen.add(label)\n",
    "    \n",
    "    network_centroids = {}\n",
    "    for i in range(0,2):\n",
    "        avg_network = np.zeros((np.shape(r_dict[0])))\n",
    "        for n in np.where(labels == unique_clusters[i])[0]:\n",
    "            avg_network = avg_network+r_dict[n]\n",
    "        avg_network = avg_network/len(np.where(labels == i))\n",
    "        network_centroids[i] = deepcopy(avg_network)\n",
    "        \n",
    "    ms_slist = s_list\n",
    "    service_type_mapping = {i: extract_service_type(label) for i, label in enumerate(ms_slist)}\n",
    "    \n",
    "    # normal_label = get_normal_label(labels)\n",
    "    # normal_mat = get_normal_mat(labels, r_dict, 0, 1)\n",
    "    normal_mat = network_centroids[0]\n",
    "    # normal_mat = remove_indices(normal_mat, non_ms, non_ms)\n",
    "    normal_mat = normal_mat/np.max(normal_mat)\n",
    "    normal_mat[np.where((normal_mat < 0.1) & (normal_mat > -0.1))] = 0\n",
    "    # normal_mat = nx.to_numpy_array(tmfg(normal_mat))\n",
    "    \n",
    "    \n",
    "    abnormal_mat = network_centroids[1]\n",
    "    # abnormal_mat = remove_indices(abnormal_mat, non_ms, non_ms)\n",
    "    abnormal_mat = abnormal_mat/np.max(abnormal_mat)\n",
    "    abnormal_mat[np.where((abnormal_mat < 0.1) & (abnormal_mat > -0.1))] = 0\n",
    "    # abnormal_mat = nx.to_numpy_array(tmfg(abnormal_mat))\n",
    "    \n",
    "    sub_G = abnormal_mat - normal_mat\n",
    "    matrix = np.abs(sub_G)\n",
    "    # matrix = pmfg(np.abs(sub_G))  # Assuming pmfg function processes the matrix appropriately\n",
    "    G = nx.from_numpy_array(sub_G)  # Create a graph from the adjacency matrix\n",
    "    \n",
    "    node_characteristics = characterize_nodes_weighted(G, walk_length=10, num_walks=100, weight='weight')\n",
    "    total_f = {n:0 for n in range(0,47)}\n",
    "    for i, characteristics in node_characteristics.items():\n",
    "        for node in characteristics:\n",
    "            total_f[node]+=characteristics[node]\n",
    "    combined_frequencies = defaultdict(float)\n",
    "    \n",
    "    i = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[i] = x\n",
    "        i+=1\n",
    "        \n",
    "    for service_id, frequency in total_f.items():\n",
    "        service_type = mapping.get(service_id, 'unknown')\n",
    "        combined_frequencies[service_type] += frequency\n",
    "    \n",
    "    # Convert the combined frequencies dictionary to a pandas dataframe\n",
    "    combined_frequencies_df = pd.DataFrame(\n",
    "        list(combined_frequencies.items()), columns=[\"Service Type\", \"Combined Frequency\"]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    ####\n",
    "    \n",
    "    t_elapsed = end_time - start_time\n",
    "    sorted_pr = combined_frequencies_df.sort_values(by=\"Combined Frequency\", ascending=False)\n",
    "    top_5 = list(sorted_pr.head(5)[\"Service Type\"])\n",
    "    hts_res_dict[experiment] = (top_5, t_elapsed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T11:14:16.158301Z",
     "start_time": "2024-12-13T11:13:21.404480Z"
    }
   },
   "id": "140fc2f2d2488a73",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fault_label = 1\n",
    "normal_label = 0\n",
    "first_index = np.where(labels == fault_label)[0][0]\n",
    "normal_indexes = np.where((labels == normal_label) & (np.arange(len(labels)) < first_index))[0]\n",
    "avg_network = np.zeros((np.shape(r_dict[0])))\n",
    "for idx in normal_indexes:\n",
    "    avg_network = avg_network + r_dict[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T11:12:19.833511Z",
     "start_time": "2024-12-13T11:12:19.830597Z"
    }
   },
   "id": "3e3605f4045afddb",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-13T10:58:21.002301Z"
    }
   },
   "id": "be734e93fe973aac",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "adff6e88824f0095"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cp_res_dict = {}\n",
    "for experiment in range(0,4):\n",
    "    with open(f\"../data/fault_data/cp_fault_{experiment}.pkl\", \"rb\") as f:\n",
    "        s_list, X = pickle.load(f)\n",
    "    ind = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[ind] = x\n",
    "        ind+=1\n",
    "    # Start CausalRCA\n",
    "    win = 360\n",
    "    step = 1\n",
    "    Xc = deepcopy(X)\n",
    "    Xdiff = np.diff(Xc[:1080,:],axis=0)\n",
    "    Xdiff = normalize(Xdiff,axis=0)\n",
    "    r_dict = dsbcorr.rolling_window(Xdiff[:,:], win, \"tapered\", step, 0.3, \"tmfg\", \"pearsons\")\n",
    "    \n",
    "    r_pos = {}\n",
    "    for corr in r_dict:\n",
    "        r_pos[corr]=deepcopy(r_dict[corr])\n",
    "        r_pos[corr] += 1\n",
    "    \n",
    "    spectra = dsbcorr.all_spectra(r_pos, True)\n",
    "    all_es = spectra.T\n",
    "    all_es = all_es[:,:-1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    landmarks = random.sample(range(0, all_es.shape[0], 1), int(len(all_es) / 4))\n",
    "    dist = distance.cdist(all_es[landmarks, :], all_es, 'euclidean')\n",
    "    coords = dsbcorr.LMDS(dist, landmarks, 2)\n",
    "    \n",
    "    n_clusters = 2\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\").fit(coords)\n",
    "    labels=clustering.labels_\n",
    "    \n",
    "    # Find the order of first instance cluster appearance\n",
    "    order_of_clusters = [key for key, _ in groupby(labels)]\n",
    "    \n",
    "    # Initialize an empty list to store unique clusters in the order of first appearance\n",
    "    unique_clusters = []\n",
    "    \n",
    "    # Set to keep track of seen clusters\n",
    "    seen = set()\n",
    "    \n",
    "    # Iterate through the labels\n",
    "    for label in labels:\n",
    "        if label not in seen:\n",
    "            unique_clusters.append(label)\n",
    "            seen.add(label)\n",
    "    \n",
    "    network_centroids = {}\n",
    "    for i in range(0,2):\n",
    "        avg_network = np.zeros((np.shape(r_dict[0])))\n",
    "        for n in np.where(labels == unique_clusters[i])[0]:\n",
    "            avg_network = avg_network+r_dict[n]\n",
    "        avg_network = avg_network/len(np.where(labels == i))\n",
    "        network_centroids[i] = deepcopy(avg_network)\n",
    "        \n",
    "    ms_slist = s_list\n",
    "    service_type_mapping = {i: extract_service_type(label) for i, label in enumerate(ms_slist)}\n",
    "    \n",
    "    # normal_label = get_normal_label(labels)\n",
    "    # normal_mat = get_normal_mat(labels, r_dict, normal_label, 1)\n",
    "    normal_mat = network_centroids[0]\n",
    "    # normal_mat = remove_indices(normal_mat, non_ms, non_ms)\n",
    "    # normal_mat = normal_mat/np.max(normal_mat)\n",
    "    normal_mat = normal_mat/len(network_centroids[0])\n",
    "    normal_mat[np.where((normal_mat < 0.1) & (normal_mat > -0.1))] = 0\n",
    "    normal_mat = nx.to_numpy_array(tmfg(normal_mat))\n",
    "    \n",
    "    \n",
    "    abnormal_mat = network_centroids[1]\n",
    "    # abnormal_mat = remove_indices(abnormal_mat, non_ms, non_ms)\n",
    "    # abnormal_mat = abnormal_mat/np.max(abnormal_mat)\n",
    "    abnormal_mat = normal_mat/len(network_centroids[1])\n",
    "    abnormal_mat[np.where((abnormal_mat < 0.1) & (abnormal_mat > -0.1))] = 0\n",
    "    abnormal_mat = nx.to_numpy_array(tmfg(abnormal_mat))\n",
    "    \n",
    "    sub_G = abnormal_mat - normal_mat\n",
    "    matrix = np.abs(sub_G)\n",
    "    # matrix = pmfg(np.abs(sub_G))  # Assuming pmfg function processes the matrix appropriately\n",
    "    G = nx.from_numpy_array(sub_G)  # Create a graph from the adjacency matrix\n",
    "    \n",
    "    node_characteristics = characterize_nodes_weighted(G, walk_length=10, num_walks=100, weight='weight')\n",
    "    total_f = {n:0 for n in range(0,47)}\n",
    "    for i, characteristics in node_characteristics.items():\n",
    "        for node in characteristics:\n",
    "            total_f[node]+=characteristics[node]\n",
    "    combined_frequencies = defaultdict(float)\n",
    "    \n",
    "    i = 0\n",
    "    mapping = {}\n",
    "    for service in s_list:\n",
    "        x = service.split('-')\n",
    "        x = x[:-2]\n",
    "        x = '-'.join(x)\n",
    "        mapping[i] = x\n",
    "        i+=1\n",
    "        \n",
    "    for service_id, frequency in total_f.items():\n",
    "        service_type = mapping.get(service_id, 'unknown')\n",
    "        combined_frequencies[service_type] += frequency\n",
    "    \n",
    "    # Convert the combined frequencies dictionary to a pandas dataframe\n",
    "    combined_frequencies_df = pd.DataFrame(\n",
    "        list(combined_frequencies.items()), columns=[\"Service Type\", \"Combined Frequency\"]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    ####\n",
    "    \n",
    "    t_elapsed = end_time - start_time\n",
    "    sorted_pr = combined_frequencies_df.sort_values(by=\"Combined Frequency\", ascending=False)\n",
    "    top_5 = list(sorted_pr.head(5)[\"Service Type\"])\n",
    "    cp_res_dict[experiment] = (top_5, t_elapsed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T08:13:10.049082Z",
     "start_time": "2024-12-14T08:12:14.443650Z"
    }
   },
   "id": "9a994dd855e7de0b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (['user-memcached', 'text-service', 'compose-post-service', 'user-service', 'post-storage-service'], 14.031043291091919), 1: (['user-service', 'media-service', 'post-storage-service', 'user-memcached', 'text-service'], 13.797024488449097), 2: (['compose-post-service', 'user-timeline-service', 'media-service', 'text-service', 'post-storage-memcached'], 13.856050491333008), 3: (['home-timeline-service', 'compose-post-service', 'url-shorten-service', 'user-memcached', 'user-mention-service'], 13.882007360458374)}\n"
     ]
    }
   ],
   "source": [
    "print(hts_res_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T17:53:39.927917Z",
     "start_time": "2024-12-11T17:53:39.925496Z"
    }
   },
   "id": "c2e10553ade3d8da",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "58c42bb14c20835b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
